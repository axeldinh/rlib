Search.setIndex({"docnames": ["agents", "envs", "index", "introduction", "learning"], "filenames": ["agents.rst", "envs.rst", "index.rst", "introduction.rst", "learning.rst"], "titles": ["Agents", "Environments", "Welcome to the documentation of my Reinforcement Learning library!", "Introduction", "Learning"], "terms": {"thi": [0, 1, 2, 3, 4], "librari": [0, 1], "contain": [0, 1, 2, 3, 4], "ar": [0, 1, 2, 3, 4], "us": [0, 2, 3, 4], "run": [0, 2, 3, 4], "differ": [0, 1, 2, 3], "algorithm": [0, 2, 3], "A": [0, 1], "function": [0, 4], "i": [0, 1, 2, 3, 4], "given": [0, 4], "creat": [0, 4], "an": [0, 2, 3, 4], "from": [0, 2, 3, 4], "configur": 0, "dictionari": [0, 2, 3], "rlib": [0, 1, 2, 3, 4], "get_ag": [0, 2, 3, 4], "obs_spac": [0, 4], "action_spac": [0, 4], "kwarg": [0, 4], "q_tabl": [0, 4], "fals": [0, 1, 4], "ddpg_q_agent": 0, "ppo_crit": 0, "global": 0, "get": [0, 4], "its": 0, "type": [0, 2, 3, 4], "paramet": [0, 2, 3, 4], "when": [0, 2, 3, 4], "The": [0, 1, 2, 3, 4], "cnn": [0, 2, 3, 4], "usual": 0, "automat": [0, 2, 3, 4], "infer": [0, 4], "environ": [0, 2, 3, 4], "": [0, 4], "observ": [0, 4], "action": [0, 2, 3, 4], "space": [0, 2, 3, 4], "exampl": [0, 4], "import": [0, 1, 2, 3, 4], "gymnasium": [0, 1, 2, 3, 4], "gym": [0, 4], "env": [0, 1, 2, 3, 4], "make": [0, 1, 2, 3, 4], "cartpol": [0, 2, 3, 4], "v1": [0, 2, 3, 4], "ha": [0, 2, 3], "box": [0, 1], "4": [0, 1, 4], "discret": [0, 2, 3, 4], "2": [0, 1, 4], "henc": [0, 4], "input_s": 0, "output_s": 0, "observation_spac": 0, "hidden_s": [0, 2, 3, 4], "64": [0, 2, 3, 4], "activ": 0, "tanh": 0, "print": [0, 4], "return": [0, 4], "layer": 0, "sequenti": 0, "0": [0, 1, 4], "linear": 0, "in_featur": 0, "out_featur": 0, "bia": 0, "true": [0, 1, 4], "1": [0, 1, 4], "relu": 0, "3": [0, 1], "dict": [0, 4], "see": [0, 2, 3], "more": [0, 2, 3], "detail": [0, 2, 3], "bool": [0, 4], "whether": [0, 4], "q": [0, 2, 3], "scalar": 0, "valu": [0, 1, 4], "each": [0, 4], "state": 0, "pair": 0, "critic": 0, "either": 0, "depend": 0, "save": [0, 2, 3, 4], "load": [0, 4], "file": 0, "class": [0, 1, 2, 3, 4], "env_kwarg": [0, 2, 3, 4], "grid_siz": [0, 4], "10": [0, 4], "tabl": [0, 4], "classic": 0, "learn": [0, 3], "size": [0, 4], "action_s": 0, "where": [0, 4], "number": [0, 4], "dimens": 0, "e": [0, 2, 3], "s_t": [0, 4], "a_t": [0, 4], "sum_": [0, 4], "k": 0, "t": [0, 4], "gamma": [0, 4], "r": [0, 4], "s_": [0, 4], "a_": 0, "discount": [0, 4], "factor": [0, 4], "end": 0, "episod": [0, 4], "time": [0, 4], "reset": 0, "get_act": 0, "updat": [0, 4], "5": [0, 1, 4], "new": [0, 4], "q_s_a": 0, "sampl": [0, 4], "q_": 0, "best_act": 0, "np": 0, "argmax": 0, "best": 0, "take": [0, 2, 3, 4], "equival": 0, "previou": 0, "line": 0, "variabl": [0, 4], "int": [0, 4], "state_s": 0, "ndarrai": 0, "__init__": [0, 4], "initi": [0, 4], "call": [0, 4], "option": [0, 4], "rais": [0, 4], "valueerror": [0, 4], "current": [0, 1, 4], "arrai": 0, "convert": 0, "tupl": 0, "integ": 0, "allow": [0, 2, 3], "none": [0, 1, 4], "otherwis": [0, 1], "float": [0, 4], "new_valu": 0, "set": [0, 1, 4], "init_weight": 0, "simpl": [0, 1], "multi": 0, "perceptron": 0, "note": [0, 2, 3], "order": 0, "evolution_strategi": [0, 4], "evolutionstrategi": [0, 2, 3], "gradient": [0, 2, 3], "should": [0, 2, 3, 4], "disabl": 0, "can": [0, 2, 3, 4], "done": [0, 2, 3, 4], "requires_grad": 0, "argument": 0, "tensor": [0, 2, 3, 4], "torch": [0, 2, 3, 4], "32": [0, 4], "hidden": 0, "neuron": 0, "x": 0, "randn": 0, "y": 0, "nn": 0, "object": 0, "input": [0, 4], "list": [0, 4], "output": 0, "str": [0, 4], "one": [0, 4], "sigmoid": 0, "default": [0, 4], "If": [0, 1, 4], "built": 1, "top": 1, "flappy_bird_gymnasium": 1, "flappy_bird_env": 1, "flappybirdenv": 1, "render_mod": [1, 4], "gap": 1, "125": 1, "observation_mod": 1, "game": [1, 4], "clone": [1, 2, 3], "origin": 1, "same": 1, "rule": [1, 4], "mechan": 1, "8": 1, "dimension": 1, "vector": 1, "follow": [1, 2, 3, 4], "num": 1, "No": 1, "flap": 1, "pass": [1, 2, 3], "pipe": 1, "There": 1, "two": [1, 2, 3, 4], "mode": 1, "imag": 1, "For": [1, 2, 3, 4], "min": 1, "max": 1, "vertic": 1, "posit": [1, 4], "veloc": 1, "inf": 1, "next": 1, "horizont": 1, "distanc": 1, "elev": 1, "bottom": 1, "6": 1, "7": 1, "512x288x3": 1, "rgb": 1, "frame": 1, "defin": 1, "drawn": 1, "screen": 1, "well": 1, "taken": [1, 4], "render": 1, "fp": 1, "flappybird": [1, 2, 3], "v0": [1, 2, 3, 4], "person": 2, "project": 2, "about": 2, "As": 2, "meant": 2, "comprehens": 2, "rather": 2, "tool": 2, "myself": 2, "To": [2, 3], "command": [2, 3], "git": [2, 3, 4], "http": [2, 3], "github": [2, 3], "com": [2, 3], "axeldinh": [2, 3], "cd": [2, 3], "pip": [2, 3], "perform": [2, 3, 4], "test": [2, 3, 4], "pytest": [2, 3], "howev": [2, 3, 4], "push": [2, 3], "doc": [2, 3], "html": [2, 3], "sphinx": [2, 3], "found": [2, 3], "build": [2, 3], "directori": [2, 3], "simpli": [2, 3], "code": [2, 3], "decompos": [2, 3], "main": [2, 3, 4], "part": [2, 3], "agent": [2, 3, 4], "which": [2, 3, 4], "have": [2, 3], "been": [2, 3], "implement": [2, 3, 4], "while": [2, 3], "interact": [2, 3], "them": [2, 3], "user": [2, 3], "must": [2, 3, 4], "choos": [2, 3], "along": [2, 3, 4], "deepqlearn": [2, 3, 4], "mlp": [2, 3, 4], "id": [2, 3, 4], "agent_kwarg": [2, 3, 4], "model": [2, 3, 4], "save_fold": [2, 3, 4], "evstrat_cartpol": [2, 3], "onli": [2, 3, 4], "ani": [2, 3, 4], "full": [2, 3], "final": [2, 3], "train": [2, 3, 4], "And": [2, 3], "plot": [2, 3, 4], "video": [2, 3, 4], "save_plot": [2, 3, 4], "save_video": [2, 3, 4], "flappi": [2, 3], "bird": [2, 3], "pygam": [2, 3], "evstrat_flappybird": [2, 3], "here": [2, 3, 4], "showcas": [2, 3], "mountaincar": [2, 3, 4], "deep": [2, 3], "lunarland": [2, 3], "v2": [2, 3], "evolut": [2, 3, 4], "strategi": [2, 3, 4], "determinist": [2, 3], "polici": [2, 3], "halfcheetah": [2, 3], "v4": [2, 3], "proxim": [2, 3], "optim": [2, 3], "bipedalwalk": [2, 3, 4], "v3": [2, 3, 4], "mani": [2, 3], "thing": [2, 3], "still": [2, 3], "miss": [2, 3], "ones": [2, 3], "gpu": [2, 3], "support": [2, 3, 4], "could": [2, 3], "easili": [2, 3], "send": [2, 3], "cuda": [2, 3], "need": [2, 3, 4], "would": [2, 3], "complex": [2, 3], "ad": [2, 3, 4], "detect": [2, 3], "index": 2, "modul": 2, "search": 2, "page": 2, "packag": 4, "all": 4, "mean": 4, "std": 4, "base_algorithm": 4, "give": 4, "baselin": 4, "useabl": 4, "our": 4, "method": 4, "train_": 4, "num_env": 4, "max_episode_length": 4, "max_total_reward": 4, "result": 4, "normalize_observ": 4, "seed": 4, "42": 4, "envs_wrapp": 4, "base": 4, "onc": 4, "env_nam": 4, "200": 4, "maximum": 4, "step": 4, "complet": 4, "reward": 4, "achiev": 4, "path": 4, "folder": 4, "videos_fold": 4, "models_fold": 4, "plots_fold": 4, "current_ag": 4, "normal": 4, "wrapper": 4, "limit": 4, "hyperparamet": 4, "info": 4, "abstract": 4, "classmethod": 4, "child": 4, "num_episod": 4, "displai": 4, "video_path": 4, "obtain": 4, "over": 4, "standard": 4, "deviat": 4, "strictli": 4, "iter": 4, "num_ag": 4, "30": 4, "num_iter": 4, "300": 4, "lr": 4, "03": 4, "sigma": 4, "test_everi": 4, "50": 4, "num_test_episod": 4, "stop_max_scor": 4, "verbos": 4, "doe": 4, "comput": 4, "therefor": 4, "compat": 4, "simplic": 4, "pytorch": 4, "network": 4, "weight": 4, "theta_": 4, "theta_t": 4, "frac": 4, "n": 4, "r_i": 4, "epsilon_i": 4, "w_t": 4, "nois": 4, "distribut": 4, "some": 4, "rate": 4, "between": 4, "plai": 4, "dure": 4, "total": 4, "stop": 4, "score": 4, "reach": 4, "progress": 4, "bar": 4, "_get_random_paramet": 4, "randomli": 4, "gener": 4, "_parameters_upd": 4, "param": 4, "test_reward": 4, "test_nois": 4, "formula": 4, "_get_test_paramet": 4, "add": 4, "q_learn": 4, "1000": 4, "99": 4, "epsilon_greedi": 4, "9": 4, "epsilon_decai": 4, "9999": 4, "epsilon_min": 4, "01": 4, "appli": 4, "alpha": 4, "left": 4, "r_": 4, "max_": 4, "right": 4, "epsilon": 4, "greedi": 4, "select": 4, "20": 4, "ommit": 4, "length": 4, "decai": 4, "minimum": 4, "deep_q_learn": 4, "deep_qlearn": 4, "0003": 4, "epsilon_start": 4, "exploration_fract": 4, "num_time_step": 4, "100000": 4, "learning_start": 4, "50000": 4, "update_everi": 4, "number_upd": 4, "main_target_upd": 4, "batch_siz": 4, "size_replay_buff": 4, "max_grad_norm": 4, "replac": 4, "neural": 4, "approxim": 4, "suitabl": 4, "store": 4, "replaybuff": 4, "befor": 4, "being": 4, "3e": 4, "probabl": 4, "random": 4, "fraction": 4, "decreas": 4, "after": 4, "100_000": 4, "start": 4, "50_000": 4, "target": 4, "100": 4, "batch": 4, "replai": 4, "buffer": 4, "norm": 4, "_populate_replay_buff": 4, "popul": 4, "until": 4, "fill": 4, "furthermor": 4, "update_weight": 4, "replay_buff": 4, "loss": 4, "ddpg": 4, "mu_kwarg": 4, "q_kwarg": 4, "q_lr": 4, "mu_lr": 4, "lr_anneal": 4, "action_nois": 4, "target_nois": 4, "num_updates_per_it": 4, "delay_policy_upd": 4, "twin_q": 4, "target_update_tau": 4, "use_norm_wrapp": 4, "improv": 4, "td3": 4, "mu": 4, "maxim": 4, "minim": 4, "becaus": 4, "natur": 4, "problem": 4, "includ": 4, "fact": 4, "differenti": 4, "continu": 4, "less": 4, "frequent": 4, "hardcor": 4, "256": 4, "1600": 4, "2_000": 4, "005": 4, "per": 4, "percentag": 4, "copi": 4, "clipact": 4, "normalizereward": 4, "clip": 4, "mujoco": 4, "notimplementederror": 4, "1d": 4, "2d": 4, "3d": 4, "_update_target_weight": 4, "tau": 4, "It": 4, "equal": 4}, "objects": {"agents.mlp": [[0, 0, 1, "", "MLP"]], "agents.mlp.MLP": [[0, 1, 1, "", "__init__"]], "agents.q_table": [[0, 0, 1, "", "QTable"]], "agents.q_table.QTable": [[0, 1, 1, "", "__init__"], [0, 1, 1, "", "discretize"], [0, 1, 1, "", "get_action"], [0, 1, 1, "", "sample"], [0, 1, 1, "", "update"]], "envs.flappy_bird_gymnasium.flappy_bird_env": [[1, 0, 1, "", "FlappyBirdEnv"]], "learning.base_algorithm": [[4, 0, 1, "", "BaseAlgorithm"]], "learning.base_algorithm.BaseAlgorithm": [[4, 1, 1, "", "__init__"], [4, 1, 1, "", "load"], [4, 1, 1, "", "save"], [4, 1, 1, "", "save_plots"], [4, 1, 1, "", "save_videos"], [4, 1, 1, "", "test"], [4, 1, 1, "", "train"], [4, 1, 1, "", "train_"]], "learning.ddpg": [[4, 0, 1, "", "DDPG"]], "learning.ddpg.DDPG": [[4, 1, 1, "", "__init__"], [4, 1, 1, "", "_populate_replay_buffer"], [4, 1, 1, "", "_update_target_weights"], [4, 1, 1, "", "update_weights"]], "learning.deep_q_learning": [[4, 0, 1, "", "DeepQLearning"]], "learning.deep_q_learning.DeepQLearning": [[4, 1, 1, "", "__init__"], [4, 1, 1, "", "_populate_replay_buffer"], [4, 1, 1, "", "update_weights"]], "learning.evolution_strategy": [[4, 0, 1, "", "EvolutionStrategy"]], "learning.evolution_strategy.EvolutionStrategy": [[4, 1, 1, "", "__init__"], [4, 1, 1, "", "_get_random_parameters"], [4, 1, 1, "", "_get_test_parameters"], [4, 1, 1, "", "_parameters_update"]], "learning.q_learning": [[4, 0, 1, "", "QLearning"]], "learning.q_learning.QLearning": [[4, 1, 1, "", "__init__"]], "rlib.agents": [[0, 2, 1, "", "get_agent"]]}, "objtypes": {"0": "py:class", "1": "py:method", "2": "py:function"}, "objnames": {"0": ["py", "class", "Python class"], "1": ["py", "method", "Python method"], "2": ["py", "function", "Python function"]}, "titleterms": {"agent": 0, "avail": [0, 4], "qtabl": 0, "mlp": 0, "environ": 1, "flappi": 1, "bird": 1, "action": 1, "space": 1, "reward": 1, "observ": 1, "debug": 1, "exampl": [1, 2, 3], "welcom": 2, "document": [2, 3], "my": 2, "reinforc": 2, "learn": [2, 4], "librari": 2, "introduct": [2, 3], "packag": [2, 3], "instal": [2, 3], "gener": [2, 3], "usag": [2, 3, 4], "limit": [2, 3], "indic": 2, "tabl": 2, "algorithm": 4, "basealgorithm": 4, "evolutionstrategi": 4, "qlearn": 4, "deep": 4, "q": 4, "determinist": 4, "polici": 4, "gradient": 4}, "envversion": {"sphinx.domains.c": 3, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 9, "sphinx.domains.index": 1, "sphinx.domains.javascript": 3, "sphinx.domains.math": 2, "sphinx.domains.python": 4, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx": 60}, "alltitles": {"Agents": [[0, "agents"]], "Available Agents:": [[0, "available-agents"]], "QTable": [[0, "qtable"]], "MLP": [[0, "mlp"]], "Environments": [[1, "environments"]], "Flappy Bird": [[1, "flappy-bird"]], "Action Space:": [[1, "action-space"]], "Reward:": [[1, "reward"]], "Observation:": [[1, "observation"]], "Debug:": [[1, "debug"]], "Example:": [[1, "example"]], "Welcome to the documentation of my Reinforcement Learning library!": [[2, "welcome-to-the-documentation-of-my-reinforcement-learning-library"]], "Introduction": [[2, "introduction"], [3, "introduction"]], "Package Installation": [[2, "package-installation"], [3, "package-installation"]], "Documentation Generation": [[2, "documentation-generation"], [3, "documentation-generation"]], "Usage": [[2, "usage"], [3, "usage"], [4, "usage"]], "Examples": [[2, "examples"], [3, "examples"]], "Limitations": [[2, "limitations"], [3, "limitations"]], "Indices and tables": [[2, "indices-and-tables"]], "Learning": [[4, "learning"]], "Algorithms Available": [[4, "algorithms-available"]], "BaseAlgorithm": [[4, "basealgorithm"]], "EvolutionStrategy": [[4, "evolutionstrategy"]], "QLearning": [[4, "qlearning"]], "Deep Q-Learning": [[4, "deep-q-learning"]], "Deep Deterministic Policy Gradient": [[4, "deep-deterministic-policy-gradient"]]}, "indexentries": {"mlp (class in agents.mlp)": [[0, "agents.mlp.MLP"]], "qtable (class in agents.q_table)": [[0, "agents.q_table.QTable"]], "__init__() (agents.mlp.mlp method)": [[0, "agents.mlp.MLP.__init__"]], "__init__() (agents.q_table.qtable method)": [[0, "agents.q_table.QTable.__init__"]], "discretize() (agents.q_table.qtable method)": [[0, "agents.q_table.QTable.discretize"]], "get_action() (agents.q_table.qtable method)": [[0, "agents.q_table.QTable.get_action"]], "get_agent() (in module rlib.agents)": [[0, "rlib.agents.get_agent"]], "sample() (agents.q_table.qtable method)": [[0, "agents.q_table.QTable.sample"]], "update() (agents.q_table.qtable method)": [[0, "agents.q_table.QTable.update"]], "flappybirdenv (class in envs.flappy_bird_gymnasium.flappy_bird_env)": [[1, "envs.flappy_bird_gymnasium.flappy_bird_env.FlappyBirdEnv"]], "basealgorithm (class in learning.base_algorithm)": [[4, "learning.base_algorithm.BaseAlgorithm"]], "ddpg (class in learning.ddpg)": [[4, "learning.ddpg.DDPG"]], "deepqlearning (class in learning.deep_q_learning)": [[4, "learning.deep_q_learning.DeepQLearning"]], "evolutionstrategy (class in learning.evolution_strategy)": [[4, "learning.evolution_strategy.EvolutionStrategy"]], "qlearning (class in learning.q_learning)": [[4, "learning.q_learning.QLearning"]], "__init__() (learning.base_algorithm.basealgorithm method)": [[4, "learning.base_algorithm.BaseAlgorithm.__init__"]], "__init__() (learning.ddpg.ddpg method)": [[4, "learning.ddpg.DDPG.__init__"]], "__init__() (learning.deep_q_learning.deepqlearning method)": [[4, "learning.deep_q_learning.DeepQLearning.__init__"]], "__init__() (learning.evolution_strategy.evolutionstrategy method)": [[4, "learning.evolution_strategy.EvolutionStrategy.__init__"]], "__init__() (learning.q_learning.qlearning method)": [[4, "learning.q_learning.QLearning.__init__"]], "_get_random_parameters() (learning.evolution_strategy.evolutionstrategy method)": [[4, "learning.evolution_strategy.EvolutionStrategy._get_random_parameters"]], "_get_test_parameters() (learning.evolution_strategy.evolutionstrategy method)": [[4, "learning.evolution_strategy.EvolutionStrategy._get_test_parameters"]], "_parameters_update() (learning.evolution_strategy.evolutionstrategy method)": [[4, "learning.evolution_strategy.EvolutionStrategy._parameters_update"]], "_populate_replay_buffer() (learning.ddpg.ddpg method)": [[4, "learning.ddpg.DDPG._populate_replay_buffer"]], "_populate_replay_buffer() (learning.deep_q_learning.deepqlearning method)": [[4, "learning.deep_q_learning.DeepQLearning._populate_replay_buffer"]], "_update_target_weights() (learning.ddpg.ddpg method)": [[4, "learning.ddpg.DDPG._update_target_weights"]], "load() (learning.base_algorithm.basealgorithm class method)": [[4, "learning.base_algorithm.BaseAlgorithm.load"]], "save() (learning.base_algorithm.basealgorithm class method)": [[4, "learning.base_algorithm.BaseAlgorithm.save"]], "save_plots() (learning.base_algorithm.basealgorithm method)": [[4, "learning.base_algorithm.BaseAlgorithm.save_plots"]], "save_videos() (learning.base_algorithm.basealgorithm method)": [[4, "learning.base_algorithm.BaseAlgorithm.save_videos"]], "test() (learning.base_algorithm.basealgorithm method)": [[4, "learning.base_algorithm.BaseAlgorithm.test"]], "train() (learning.base_algorithm.basealgorithm method)": [[4, "learning.base_algorithm.BaseAlgorithm.train"]], "train_() (learning.base_algorithm.basealgorithm class method)": [[4, "learning.base_algorithm.BaseAlgorithm.train_"]], "update_weights() (learning.ddpg.ddpg method)": [[4, "learning.ddpg.DDPG.update_weights"]], "update_weights() (learning.deep_q_learning.deepqlearning method)": [[4, "learning.deep_q_learning.DeepQLearning.update_weights"]]}})