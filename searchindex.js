Search.setIndex({"docnames": ["Introduction", "agents", "envs", "index", "learning"], "filenames": ["Introduction.rst", "agents.rst", "envs.rst", "index.rst", "learning.rst"], "titles": ["Introduction", "Agents", "Environments", "Welcome to RLib\u2019s documentation!", "Learning"], "terms": {"To": 0, "run": [0, 1, 4], "follow": [0, 2, 4], "command": 0, "git": [0, 4], "clone": [0, 2], "http": 0, "github": 0, "com": 0, "axeldinh": 0, "rlib": [0, 1, 2, 4], "cd": 0, "pip": 0, "e": [0, 1], "perform": [0, 4], "test": [0, 4], "pytest": 0, "howev": [0, 4], "ar": [0, 1, 2, 4], "automat": [0, 1, 4], "push": 0, "us": [0, 1, 4], "action": [0, 1, 4], "doc": 0, "make": [0, 1, 2, 4], "html": 0, "note": [0, 1], "i": [0, 1, 2, 4], "sphinx": 0, "The": [0, 1, 2, 4], "can": [0, 1, 4], "found": 0, "build": 0, "directori": 0, "simpli": 0, "import": [0, 1, 2, 4], "code": 0, "decompos": 0, "two": [0, 2, 4], "main": [0, 4], "part": 0, "learn": [0, 1, 3], "agent": [0, 3, 4], "contain": [0, 1, 2, 4], "differ": [0, 1, 2], "algorithm": [0, 1, 3], "which": [0, 4], "have": 0, "been": 0, "implement": [0, 4], "while": 0, "interact": 0, "environ": [0, 1, 3, 4], "them": 0, "gymnasium": [0, 1, 2, 4], "user": 0, "must": [0, 4], "choos": 0, "an": [0, 1, 4], "along": [0, 4], "paramet": [0, 1, 4], "should": [0, 1, 4], "take": [0, 1, 4], "For": [0, 2, 4], "deepqlearn": [0, 4], "cartpol": [0, 1, 4], "v1": [0, 1, 4], "space": [0, 1, 4], "discret": [0, 1, 4], "mlp": [0, 3, 4], "thi": [0, 1, 2, 4], "done": [0, 1, 4], "from": [0, 1, 2, 4], "evolutionstrategi": [0, 1, 3], "env_kwarg": [0, 1, 4], "id": [0, 4], "agent_kwarg": [0, 4], "hidden_s": [0, 1, 4], "64": [0, 1, 4], "model": [0, 4], "save_fold": [0, 4], "evstrat_cartpol": 0, "onli": [0, 4], "dictionari": [0, 1], "pass": [0, 2], "allow": [0, 1], "ani": [0, 4], "full": 0, "save": [0, 1, 4], "final": 0, "train": [0, 4], "And": 0, "plot": [0, 4], "video": [0, 4], "save_plot": [0, 4], "save_video": [0, 4], "here": [0, 4], "showcas": 0, "q": [0, 1, 3], "mountaincar": [0, 4], "v0": [0, 2, 4], "deep": [0, 3], "lunarland": 0, "v2": 0, "evolut": [0, 4], "strategi": [0, 4], "flappybird": [0, 2], "determinist": [0, 3], "polici": [0, 3], "gradient": [0, 1, 3], "halfcheetah": 0, "v4": 0, "proxim": 0, "optim": 0, "bipedalwalk": [0, 4], "v3": [0, 4], "librari": [1, 2], "A": [1, 2], "function": [1, 4], "given": [1, 4], "creat": [1, 4], "configur": 1, "get_ag": [1, 4], "obs_spac": [1, 4], "action_spac": [1, 4], "kwarg": [1, 4], "q_tabl": [1, 4], "fals": [1, 2, 4], "ddpg_q_agent": 1, "ppo_crit": 1, "global": 1, "get": [1, 4], "its": 1, "type": [1, 4], "when": [1, 4], "cnn": [1, 4], "usual": 1, "infer": [1, 4], "": [1, 4], "observ": [1, 4], "exampl": [1, 4], "gym": [1, 4], "env": [1, 2, 4], "ha": 1, "box": [1, 2], "4": [1, 2, 4], "2": [1, 2, 4], "henc": [1, 4], "input_s": 1, "output_s": 1, "observation_spac": 1, "activ": 1, "tanh": 1, "print": [1, 4], "return": [1, 4], "layer": 1, "sequenti": 1, "0": [1, 2, 4], "linear": 1, "in_featur": 1, "out_featur": 1, "bia": 1, "true": [1, 2, 4], "1": [1, 2, 4], "relu": 1, "3": [1, 2], "dict": [1, 4], "see": 1, "more": 1, "detail": 1, "bool": [1, 4], "whether": [1, 4], "scalar": 1, "valu": [1, 2, 4], "each": [1, 4], "state": 1, "pair": 1, "critic": 1, "either": 1, "depend": 1, "load": [1, 4], "file": 1, "class": [1, 2, 4], "grid_siz": [1, 4], "10": [1, 4], "tabl": [1, 4], "classic": 1, "size": [1, 4], "action_s": 1, "where": [1, 4], "number": [1, 4], "dimens": 1, "s_t": [1, 4], "a_t": [1, 4], "sum_": [1, 4], "k": 1, "t": [1, 4], "gamma": [1, 4], "r": [1, 4], "s_": [1, 4], "a_": 1, "discount": [1, 4], "factor": [1, 4], "end": 1, "episod": [1, 4], "time": [1, 4], "reset": 1, "get_act": 1, "updat": [1, 4], "5": [1, 2, 4], "new": [1, 4], "q_s_a": 1, "sampl": [1, 4], "q_": 1, "best_act": 1, "np": 1, "argmax": 1, "best": 1, "equival": 1, "previou": 1, "line": 1, "variabl": [1, 4], "int": [1, 4], "state_s": 1, "ndarrai": 1, "__init__": [1, 4], "initi": [1, 4], "call": [1, 4], "option": [1, 4], "rais": [1, 4], "valueerror": [1, 4], "current": [1, 2, 4], "arrai": 1, "convert": 1, "tupl": 1, "integ": 1, "none": [1, 2, 4], "otherwis": [1, 2], "float": [1, 4], "new_valu": 1, "set": [1, 2, 4], "init_weight": 1, "simpl": [1, 2], "multi": 1, "perceptron": 1, "order": 1, "evolution_strategi": [1, 4], "disabl": 1, "requires_grad": 1, "argument": 1, "tensor": [1, 4], "torch": [1, 4], "32": [1, 4], "hidden": 1, "neuron": 1, "x": 1, "randn": 1, "y": 1, "nn": 1, "object": 1, "input": [1, 4], "list": [1, 4], "output": 1, "str": [1, 4], "one": [1, 4], "sigmoid": 1, "default": [1, 4], "If": [1, 2, 4], "built": 2, "top": 2, "flappy_bird_gymnasium": 2, "flappy_bird_env": 2, "flappybirdenv": 2, "render_mod": [2, 4], "gap": 2, "125": 2, "observation_mod": 2, "game": [2, 4], "origin": 2, "same": 2, "rule": [2, 4], "mechan": 2, "8": 2, "dimension": 2, "vector": 2, "num": 2, "No": 2, "flap": 2, "pipe": 2, "There": 2, "mode": 2, "imag": 2, "min": 2, "max": 2, "vertic": 2, "posit": [2, 4], "veloc": 2, "inf": 2, "next": 2, "horizont": 2, "distanc": 2, "elev": 2, "bottom": 2, "6": 2, "7": 2, "512x288x3": 2, "rgb": 2, "frame": 2, "defin": 2, "drawn": 2, "screen": 2, "well": 2, "taken": [2, 4], "render": 2, "fp": 2, "avail": 3, "usag": 3, "basealgorithm": 3, "qlearn": 3, "qtabl": 3, "flappi": 3, "bird": 3, "index": 3, "modul": 3, "search": 3, "page": 3, "packag": 4, "all": 4, "mean": 4, "std": 4, "base_algorithm": 4, "give": 4, "baselin": 4, "useabl": 4, "our": 4, "method": 4, "train_": 4, "num_env": 4, "max_episode_length": 4, "max_total_reward": 4, "result": 4, "normalize_observ": 4, "seed": 4, "42": 4, "envs_wrapp": 4, "base": 4, "onc": 4, "env_nam": 4, "200": 4, "maximum": 4, "step": 4, "complet": 4, "reward": 4, "achiev": 4, "path": 4, "folder": 4, "videos_fold": 4, "models_fold": 4, "plots_fold": 4, "current_ag": 4, "normal": 4, "wrapper": 4, "limit": 4, "hyperparamet": 4, "info": 4, "abstract": 4, "classmethod": 4, "child": 4, "num_episod": 4, "displai": 4, "video_path": 4, "obtain": 4, "over": 4, "standard": 4, "deviat": 4, "strictli": 4, "iter": 4, "num_ag": 4, "30": 4, "num_iter": 4, "300": 4, "lr": 4, "03": 4, "sigma": 4, "test_everi": 4, "50": 4, "num_test_episod": 4, "stop_max_scor": 4, "verbos": 4, "doe": 4, "need": 4, "comput": 4, "therefor": 4, "compat": 4, "simplic": 4, "pytorch": 4, "network": 4, "weight": 4, "theta_": 4, "theta_t": 4, "frac": 4, "n": 4, "r_i": 4, "epsilon_i": 4, "w_t": 4, "nois": 4, "distribut": 4, "some": 4, "rate": 4, "between": 4, "plai": 4, "dure": 4, "total": 4, "stop": 4, "score": 4, "reach": 4, "progress": 4, "bar": 4, "_get_random_paramet": 4, "randomli": 4, "gener": 4, "_parameters_upd": 4, "param": 4, "test_reward": 4, "test_nois": 4, "formula": 4, "_get_test_paramet": 4, "add": 4, "q_learn": 4, "1000": 4, "99": 4, "epsilon_greedi": 4, "9": 4, "epsilon_decai": 4, "9999": 4, "epsilon_min": 4, "01": 4, "appli": 4, "alpha": 4, "left": 4, "r_": 4, "max_": 4, "right": 4, "epsilon": 4, "greedi": 4, "select": 4, "20": 4, "ommit": 4, "length": 4, "decai": 4, "minimum": 4, "deep_q_learn": 4, "deep_qlearn": 4, "0003": 4, "epsilon_start": 4, "exploration_fract": 4, "num_time_step": 4, "100000": 4, "learning_start": 4, "50000": 4, "update_everi": 4, "number_upd": 4, "main_target_upd": 4, "batch_siz": 4, "size_replay_buff": 4, "max_grad_norm": 4, "replac": 4, "neural": 4, "approxim": 4, "suitabl": 4, "store": 4, "replaybuff": 4, "befor": 4, "being": 4, "3e": 4, "probabl": 4, "random": 4, "fraction": 4, "decreas": 4, "after": 4, "100_000": 4, "start": 4, "50_000": 4, "target": 4, "100": 4, "batch": 4, "replai": 4, "buffer": 4, "norm": 4, "_populate_replay_buff": 4, "popul": 4, "until": 4, "fill": 4, "furthermor": 4, "update_weight": 4, "replay_buff": 4, "loss": 4, "ddpg": 4, "mu_kwarg": 4, "q_kwarg": 4, "q_lr": 4, "mu_lr": 4, "lr_anneal": 4, "action_nois": 4, "target_nois": 4, "num_updates_per_it": 4, "delay_policy_upd": 4, "twin_q": 4, "target_update_tau": 4, "use_norm_wrapp": 4, "improv": 4, "td3": 4, "mu": 4, "maxim": 4, "minim": 4, "becaus": 4, "natur": 4, "problem": 4, "includ": 4, "fact": 4, "differenti": 4, "continu": 4, "support": 4, "less": 4, "frequent": 4, "hardcor": 4, "256": 4, "1600": 4, "2_000": 4, "005": 4, "ad": 4, "per": 4, "percentag": 4, "copi": 4, "clipact": 4, "normalizereward": 4, "clip": 4, "mujoco": 4, "notimplementederror": 4, "1d": 4, "2d": 4, "3d": 4, "_update_target_weight": 4, "tau": 4, "It": 4, "equal": 4}, "objects": {"agents.mlp": [[1, 0, 1, "", "MLP"]], "agents.mlp.MLP": [[1, 1, 1, "", "__init__"]], "agents.q_table": [[1, 0, 1, "", "QTable"]], "agents.q_table.QTable": [[1, 1, 1, "", "__init__"], [1, 1, 1, "", "discretize"], [1, 1, 1, "", "get_action"], [1, 1, 1, "", "sample"], [1, 1, 1, "", "update"]], "envs.flappy_bird_gymnasium.flappy_bird_env": [[2, 0, 1, "", "FlappyBirdEnv"]], "learning.base_algorithm": [[4, 0, 1, "", "BaseAlgorithm"]], "learning.base_algorithm.BaseAlgorithm": [[4, 1, 1, "", "__init__"], [4, 1, 1, "", "load"], [4, 1, 1, "", "save"], [4, 1, 1, "", "save_plots"], [4, 1, 1, "", "save_videos"], [4, 1, 1, "", "test"], [4, 1, 1, "", "train"], [4, 1, 1, "", "train_"]], "learning.ddpg": [[4, 0, 1, "", "DDPG"]], "learning.ddpg.DDPG": [[4, 1, 1, "", "__init__"], [4, 1, 1, "", "_populate_replay_buffer"], [4, 1, 1, "", "_update_target_weights"], [4, 1, 1, "", "update_weights"]], "learning.deep_q_learning": [[4, 0, 1, "", "DeepQLearning"]], "learning.deep_q_learning.DeepQLearning": [[4, 1, 1, "", "__init__"], [4, 1, 1, "", "_populate_replay_buffer"], [4, 1, 1, "", "update_weights"]], "learning.evolution_strategy": [[4, 0, 1, "", "EvolutionStrategy"]], "learning.evolution_strategy.EvolutionStrategy": [[4, 1, 1, "", "__init__"], [4, 1, 1, "", "_get_random_parameters"], [4, 1, 1, "", "_get_test_parameters"], [4, 1, 1, "", "_parameters_update"]], "learning.q_learning": [[4, 0, 1, "", "QLearning"]], "learning.q_learning.QLearning": [[4, 1, 1, "", "__init__"]], "rlib.agents": [[1, 2, 1, "", "get_agent"]]}, "objtypes": {"0": "py:class", "1": "py:method", "2": "py:function"}, "objnames": {"0": ["py", "class", "Python class"], "1": ["py", "method", "Python method"], "2": ["py", "function", "Python function"]}, "titleterms": {"introduct": 0, "packag": 0, "instal": 0, "document": [0, 3], "gener": 0, "usag": [0, 4], "exampl": [0, 2], "agent": 1, "avail": [1, 4], "qtabl": 1, "mlp": 1, "environ": 2, "flappi": 2, "bird": 2, "action": 2, "space": 2, "reward": 2, "observ": 2, "debug": 2, "welcom": 3, "rlib": 3, "": 3, "content": 3, "indic": 3, "tabl": 3, "learn": 4, "algorithm": 4, "basealgorithm": 4, "evolutionstrategi": 4, "qlearn": 4, "deep": 4, "q": 4, "determinist": 4, "polici": 4, "gradient": 4}, "envversion": {"sphinx.domains.c": 3, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 9, "sphinx.domains.index": 1, "sphinx.domains.javascript": 3, "sphinx.domains.math": 2, "sphinx.domains.python": 4, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx": 60}, "alltitles": {"Introduction": [[0, "introduction"]], "Package Installation": [[0, "package-installation"]], "Documentation Generation": [[0, "documentation-generation"]], "Usage": [[0, "usage"], [4, "usage"]], "Examples": [[0, "examples"]], "Agents": [[1, "agents"]], "Available Agents:": [[1, "available-agents"]], "QTable": [[1, "qtable"]], "MLP": [[1, "mlp"]], "Environments": [[2, "environments"]], "Flappy Bird": [[2, "flappy-bird"]], "Action Space:": [[2, "action-space"]], "Reward:": [[2, "reward"]], "Observation:": [[2, "observation"]], "Debug:": [[2, "debug"]], "Example:": [[2, "example"]], "Welcome to RLib\u2019s documentation!": [[3, "welcome-to-rlib-s-documentation"]], "Contents:": [[3, null]], "Indices and tables": [[3, "indices-and-tables"]], "Learning": [[4, "learning"]], "Algorithms Available": [[4, "algorithms-available"]], "BaseAlgorithm": [[4, "basealgorithm"]], "EvolutionStrategy": [[4, "evolutionstrategy"]], "QLearning": [[4, "qlearning"]], "Deep Q-Learning": [[4, "deep-q-learning"]], "Deep Deterministic Policy Gradient": [[4, "deep-deterministic-policy-gradient"]]}, "indexentries": {"mlp (class in agents.mlp)": [[1, "agents.mlp.MLP"]], "qtable (class in agents.q_table)": [[1, "agents.q_table.QTable"]], "__init__() (agents.mlp.mlp method)": [[1, "agents.mlp.MLP.__init__"]], "__init__() (agents.q_table.qtable method)": [[1, "agents.q_table.QTable.__init__"]], "discretize() (agents.q_table.qtable method)": [[1, "agents.q_table.QTable.discretize"]], "get_action() (agents.q_table.qtable method)": [[1, "agents.q_table.QTable.get_action"]], "get_agent() (in module rlib.agents)": [[1, "rlib.agents.get_agent"]], "sample() (agents.q_table.qtable method)": [[1, "agents.q_table.QTable.sample"]], "update() (agents.q_table.qtable method)": [[1, "agents.q_table.QTable.update"]], "flappybirdenv (class in envs.flappy_bird_gymnasium.flappy_bird_env)": [[2, "envs.flappy_bird_gymnasium.flappy_bird_env.FlappyBirdEnv"]], "basealgorithm (class in learning.base_algorithm)": [[4, "learning.base_algorithm.BaseAlgorithm"]], "ddpg (class in learning.ddpg)": [[4, "learning.ddpg.DDPG"]], "deepqlearning (class in learning.deep_q_learning)": [[4, "learning.deep_q_learning.DeepQLearning"]], "evolutionstrategy (class in learning.evolution_strategy)": [[4, "learning.evolution_strategy.EvolutionStrategy"]], "qlearning (class in learning.q_learning)": [[4, "learning.q_learning.QLearning"]], "__init__() (learning.base_algorithm.basealgorithm method)": [[4, "learning.base_algorithm.BaseAlgorithm.__init__"]], "__init__() (learning.ddpg.ddpg method)": [[4, "learning.ddpg.DDPG.__init__"]], "__init__() (learning.deep_q_learning.deepqlearning method)": [[4, "learning.deep_q_learning.DeepQLearning.__init__"]], "__init__() (learning.evolution_strategy.evolutionstrategy method)": [[4, "learning.evolution_strategy.EvolutionStrategy.__init__"]], "__init__() (learning.q_learning.qlearning method)": [[4, "learning.q_learning.QLearning.__init__"]], "_get_random_parameters() (learning.evolution_strategy.evolutionstrategy method)": [[4, "learning.evolution_strategy.EvolutionStrategy._get_random_parameters"]], "_get_test_parameters() (learning.evolution_strategy.evolutionstrategy method)": [[4, "learning.evolution_strategy.EvolutionStrategy._get_test_parameters"]], "_parameters_update() (learning.evolution_strategy.evolutionstrategy method)": [[4, "learning.evolution_strategy.EvolutionStrategy._parameters_update"]], "_populate_replay_buffer() (learning.ddpg.ddpg method)": [[4, "learning.ddpg.DDPG._populate_replay_buffer"]], "_populate_replay_buffer() (learning.deep_q_learning.deepqlearning method)": [[4, "learning.deep_q_learning.DeepQLearning._populate_replay_buffer"]], "_update_target_weights() (learning.ddpg.ddpg method)": [[4, "learning.ddpg.DDPG._update_target_weights"]], "load() (learning.base_algorithm.basealgorithm class method)": [[4, "learning.base_algorithm.BaseAlgorithm.load"]], "save() (learning.base_algorithm.basealgorithm class method)": [[4, "learning.base_algorithm.BaseAlgorithm.save"]], "save_plots() (learning.base_algorithm.basealgorithm method)": [[4, "learning.base_algorithm.BaseAlgorithm.save_plots"]], "save_videos() (learning.base_algorithm.basealgorithm method)": [[4, "learning.base_algorithm.BaseAlgorithm.save_videos"]], "test() (learning.base_algorithm.basealgorithm method)": [[4, "learning.base_algorithm.BaseAlgorithm.test"]], "train() (learning.base_algorithm.basealgorithm method)": [[4, "learning.base_algorithm.BaseAlgorithm.train"]], "train_() (learning.base_algorithm.basealgorithm class method)": [[4, "learning.base_algorithm.BaseAlgorithm.train_"]], "update_weights() (learning.ddpg.ddpg method)": [[4, "learning.ddpg.DDPG.update_weights"]], "update_weights() (learning.deep_q_learning.deepqlearning method)": [[4, "learning.deep_q_learning.DeepQLearning.update_weights"]]}})