Search.setIndex({"docnames": ["agents", "envs", "index", "learning"], "filenames": ["agents.rst", "envs.rst", "index.rst", "learning.rst"], "titles": ["Agents", "Environments", "Welcome to RLib\u2019s documentation!", "Learning"], "terms": {"thi": [0, 1, 3], "librari": [0, 1], "contain": [0, 1, 3], "ar": [0, 1, 3], "us": [0, 3], "run": [0, 3], "differ": [0, 1], "algorithm": [0, 2], "A": [0, 1], "function": [0, 3], "i": [0, 1, 3], "given": [0, 3], "creat": [0, 3], "an": [0, 3], "from": [0, 1, 3], "configur": 0, "dictionari": 0, "rlib": [0, 1, 3], "get_ag": [0, 3], "obs_spac": [0, 3], "action_spac": [0, 3], "kwarg": [0, 3], "q_tabl": [0, 3], "fals": [0, 1, 3], "ddpg_q_agent": 0, "ppo_crit": 0, "global": 0, "get": [0, 3], "its": 0, "type": [0, 3], "paramet": [0, 3], "when": [0, 3], "The": [0, 1, 3], "cnn": [0, 3], "usual": 0, "automat": [0, 3], "infer": [0, 3], "environ": [0, 2, 3], "": [0, 3], "observ": [0, 3], "action": [0, 3], "space": [0, 3], "exampl": [0, 3], "import": [0, 1, 3], "gymnasium": [0, 1, 3], "gym": [0, 3], "env": [0, 1, 3], "make": [0, 1, 3], "cartpol": [0, 3], "v1": [0, 3], "ha": 0, "box": [0, 1], "4": [0, 1, 3], "discret": [0, 3], "2": [0, 1, 3], "henc": [0, 3], "input_s": 0, "output_s": 0, "observation_spac": 0, "hidden_s": [0, 3], "64": [0, 3], "activ": 0, "tanh": 0, "print": [0, 3], "return": [0, 3], "layer": 0, "sequenti": 0, "0": [0, 1, 3], "linear": 0, "in_featur": 0, "out_featur": 0, "bia": 0, "true": [0, 1, 3], "1": [0, 1, 3], "relu": 0, "3": [0, 1], "dict": [0, 3], "see": 0, "more": 0, "detail": 0, "bool": [0, 3], "whether": [0, 3], "q": [0, 2], "scalar": 0, "valu": [0, 1, 3], "each": [0, 3], "state": 0, "pair": 0, "critic": 0, "either": 0, "depend": 0, "save": [0, 3], "load": [0, 3], "file": 0, "class": [0, 1, 3], "env_kwarg": [0, 3], "grid_siz": [0, 3], "10": [0, 3], "tabl": [0, 3], "classic": 0, "learn": [0, 2], "size": [0, 3], "action_s": 0, "where": [0, 3], "number": [0, 3], "dimens": 0, "e": 0, "s_t": [0, 3], "a_t": [0, 3], "sum_": [0, 3], "k": 0, "t": [0, 3], "gamma": [0, 3], "r": [0, 3], "s_": [0, 3], "a_": 0, "discount": [0, 3], "factor": [0, 3], "end": 0, "episod": [0, 3], "time": [0, 3], "reset": 0, "get_act": 0, "updat": [0, 3], "5": [0, 1, 3], "new": [0, 3], "q_s_a": 0, "sampl": [0, 3], "q_": 0, "best_act": 0, "np": 0, "argmax": 0, "best": 0, "take": [0, 3], "equival": 0, "previou": 0, "line": 0, "variabl": [0, 3], "int": [0, 3], "state_s": 0, "ndarrai": 0, "__init__": [0, 3], "initi": [0, 3], "call": [0, 3], "option": [0, 3], "rais": [0, 3], "valueerror": [0, 3], "current": [0, 1, 3], "arrai": 0, "convert": 0, "tupl": 0, "integ": 0, "allow": 0, "none": [0, 1, 3], "otherwis": [0, 1], "float": [0, 3], "new_valu": 0, "set": [0, 1, 3], "init_weight": 0, "simpl": [0, 1], "multi": 0, "perceptron": 0, "note": 0, "order": 0, "evolution_strategi": [0, 3], "evolutionstrategi": [0, 2], "gradient": [0, 2], "should": [0, 3], "disabl": 0, "can": [0, 3], "done": [0, 3], "requires_grad": 0, "argument": 0, "tensor": [0, 3], "torch": [0, 3], "32": [0, 3], "hidden": 0, "neuron": 0, "x": 0, "randn": 0, "y": 0, "nn": 0, "object": 0, "input": [0, 3], "list": [0, 3], "output": 0, "str": [0, 3], "one": [0, 3], "sigmoid": 0, "default": [0, 3], "If": [0, 1, 3], "built": 1, "top": 1, "flappy_bird_gymnasium": 1, "flappy_bird_env": 1, "flappybirdenv": 1, "render_mod": [1, 3], "gap": 1, "125": 1, "observation_mod": 1, "game": [1, 3], "clone": 1, "origin": 1, "same": 1, "rule": [1, 3], "mechan": 1, "8": 1, "dimension": 1, "vector": 1, "follow": [1, 3], "num": 1, "No": 1, "flap": 1, "pass": 1, "pipe": 1, "There": 1, "two": [1, 3], "mode": 1, "imag": 1, "For": [1, 3], "min": 1, "max": 1, "vertic": 1, "posit": [1, 3], "veloc": 1, "inf": 1, "next": 1, "horizont": 1, "distanc": 1, "elev": 1, "bottom": 1, "6": 1, "7": 1, "512x288x3": 1, "rgb": 1, "frame": 1, "defin": 1, "drawn": 1, "screen": 1, "well": 1, "taken": [1, 3], "render": 1, "fp": 1, "flappybird": 1, "v0": [1, 3], "avail": 2, "usag": 2, "basealgorithm": 2, "qlearn": 2, "deep": 2, "determinist": 2, "polici": 2, "agent": [2, 3], "qtabl": 2, "mlp": [2, 3], "flappi": 2, "bird": 2, "index": 2, "modul": 2, "search": 2, "page": 2, "packag": 3, "implement": 3, "all": 3, "which": 3, "id": 3, "agent_kwarg": 3, "train": 3, "mean": 3, "std": 3, "test": 3, "save_plot": 3, "save_video": 3, "base_algorithm": 3, "give": 3, "baselin": 3, "useabl": 3, "our": 3, "must": 3, "method": 3, "train_": 3, "num_env": 3, "max_episode_length": 3, "max_total_reward": 3, "save_fold": 3, "result": 3, "normalize_observ": 3, "seed": 3, "42": 3, "envs_wrapp": 3, "base": 3, "onc": 3, "deepqlearn": 3, "env_nam": 3, "200": 3, "model": 3, "maximum": 3, "step": 3, "complet": 3, "reward": 3, "achiev": 3, "path": 3, "folder": 3, "videos_fold": 3, "video": 3, "models_fold": 3, "plots_fold": 3, "plot": 3, "current_ag": 3, "normal": 3, "wrapper": 3, "limit": 3, "along": 3, "hyperparamet": 3, "git": 3, "info": 3, "abstract": 3, "classmethod": 3, "child": 3, "num_episod": 3, "displai": 3, "video_path": 3, "obtain": 3, "over": 3, "standard": 3, "deviat": 3, "strictli": 3, "iter": 3, "num_ag": 3, "30": 3, "num_iter": 3, "300": 3, "lr": 3, "03": 3, "sigma": 3, "test_everi": 3, "50": 3, "num_test_episod": 3, "stop_max_scor": 3, "verbos": 3, "evolut": 3, "strategi": 3, "doe": 3, "need": 3, "comput": 3, "therefor": 3, "compat": 3, "ani": 3, "howev": 3, "simplic": 3, "pytorch": 3, "network": 3, "here": 3, "weight": 3, "theta_": 3, "theta_t": 3, "frac": 3, "n": 3, "r_i": 3, "epsilon_i": 3, "w_t": 3, "nois": 3, "distribut": 3, "some": 3, "rate": 3, "between": 3, "plai": 3, "dure": 3, "total": 3, "stop": 3, "score": 3, "reach": 3, "progress": 3, "bar": 3, "_get_random_paramet": 3, "randomli": 3, "gener": 3, "_parameters_upd": 3, "param": 3, "test_reward": 3, "test_nois": 3, "formula": 3, "_get_test_paramet": 3, "add": 3, "q_learn": 3, "1000": 3, "99": 3, "epsilon_greedi": 3, "9": 3, "epsilon_decai": 3, "9999": 3, "epsilon_min": 3, "01": 3, "appli": 3, "alpha": 3, "left": 3, "r_": 3, "max_": 3, "right": 3, "epsilon": 3, "greedi": 3, "select": 3, "mountaincar": 3, "20": 3, "ommit": 3, "length": 3, "decai": 3, "minimum": 3, "deep_q_learn": 3, "deep_qlearn": 3, "0003": 3, "epsilon_start": 3, "exploration_fract": 3, "num_time_step": 3, "100000": 3, "learning_start": 3, "50000": 3, "update_everi": 3, "number_upd": 3, "main_target_upd": 3, "batch_siz": 3, "size_replay_buff": 3, "max_grad_norm": 3, "replac": 3, "neural": 3, "approxim": 3, "onli": 3, "suitabl": 3, "store": 3, "replaybuff": 3, "befor": 3, "being": 3, "3e": 3, "probabl": 3, "random": 3, "fraction": 3, "decreas": 3, "after": 3, "100_000": 3, "start": 3, "50_000": 3, "perform": 3, "target": 3, "100": 3, "batch": 3, "replai": 3, "buffer": 3, "norm": 3, "_populate_replay_buff": 3, "popul": 3, "until": 3, "fill": 3, "furthermor": 3, "update_weight": 3, "replay_buff": 3, "loss": 3, "ddpg": 3, "mu_kwarg": 3, "q_kwarg": 3, "q_lr": 3, "mu_lr": 3, "lr_anneal": 3, "action_nois": 3, "target_nois": 3, "num_updates_per_it": 3, "delay_policy_upd": 3, "twin_q": 3, "target_update_tau": 3, "use_norm_wrapp": 3, "improv": 3, "td3": 3, "mu": 3, "maxim": 3, "minim": 3, "becaus": 3, "natur": 3, "problem": 3, "includ": 3, "fact": 3, "differenti": 3, "continu": 3, "support": 3, "less": 3, "frequent": 3, "bipedalwalk": 3, "v3": 3, "hardcor": 3, "256": 3, "1600": 3, "2_000": 3, "005": 3, "ad": 3, "per": 3, "percentag": 3, "copi": 3, "main": 3, "clipact": 3, "normalizereward": 3, "clip": 3, "mujoco": 3, "notimplementederror": 3, "1d": 3, "2d": 3, "3d": 3, "_update_target_weight": 3, "tau": 3, "It": 3, "equal": 3}, "objects": {"agents.mlp": [[0, 0, 1, "", "MLP"]], "agents.mlp.MLP": [[0, 1, 1, "", "__init__"]], "agents.q_table": [[0, 0, 1, "", "QTable"]], "agents.q_table.QTable": [[0, 1, 1, "", "__init__"], [0, 1, 1, "", "discretize"], [0, 1, 1, "", "get_action"], [0, 1, 1, "", "sample"], [0, 1, 1, "", "update"]], "envs.flappy_bird_gymnasium.flappy_bird_env": [[1, 0, 1, "", "FlappyBirdEnv"]], "learning.base_algorithm": [[3, 0, 1, "", "BaseAlgorithm"]], "learning.base_algorithm.BaseAlgorithm": [[3, 1, 1, "", "__init__"], [3, 1, 1, "", "load"], [3, 1, 1, "", "save"], [3, 1, 1, "", "save_plots"], [3, 1, 1, "", "save_videos"], [3, 1, 1, "", "test"], [3, 1, 1, "", "train"], [3, 1, 1, "", "train_"]], "learning.ddpg": [[3, 0, 1, "", "DDPG"]], "learning.ddpg.DDPG": [[3, 1, 1, "", "__init__"], [3, 1, 1, "", "_populate_replay_buffer"], [3, 1, 1, "", "_update_target_weights"], [3, 1, 1, "", "update_weights"]], "learning.deep_q_learning": [[3, 0, 1, "", "DeepQLearning"]], "learning.deep_q_learning.DeepQLearning": [[3, 1, 1, "", "__init__"], [3, 1, 1, "", "_populate_replay_buffer"], [3, 1, 1, "", "update_weights"]], "learning.evolution_strategy": [[3, 0, 1, "", "EvolutionStrategy"]], "learning.evolution_strategy.EvolutionStrategy": [[3, 1, 1, "", "__init__"], [3, 1, 1, "", "_get_random_parameters"], [3, 1, 1, "", "_get_test_parameters"], [3, 1, 1, "", "_parameters_update"]], "learning.q_learning": [[3, 0, 1, "", "QLearning"]], "learning.q_learning.QLearning": [[3, 1, 1, "", "__init__"]], "rlib.agents": [[0, 2, 1, "", "get_agent"]]}, "objtypes": {"0": "py:class", "1": "py:method", "2": "py:function"}, "objnames": {"0": ["py", "class", "Python class"], "1": ["py", "method", "Python method"], "2": ["py", "function", "Python function"]}, "titleterms": {"agent": 0, "avail": [0, 3], "qtabl": 0, "mlp": 0, "environ": 1, "flappi": 1, "bird": 1, "action": 1, "space": 1, "reward": 1, "observ": 1, "debug": 1, "exampl": 1, "welcom": 2, "rlib": 2, "": 2, "document": 2, "content": 2, "indic": 2, "tabl": 2, "learn": 3, "algorithm": 3, "usag": 3, "basealgorithm": 3, "evolutionstrategi": 3, "qlearn": 3, "deep": 3, "q": 3, "determinist": 3, "polici": 3, "gradient": 3}, "envversion": {"sphinx.domains.c": 3, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 9, "sphinx.domains.index": 1, "sphinx.domains.javascript": 3, "sphinx.domains.math": 2, "sphinx.domains.python": 4, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx": 60}, "alltitles": {"Agents": [[0, "agents"]], "Available Agents:": [[0, "available-agents"]], "QTable": [[0, "qtable"]], "MLP": [[0, "mlp"]], "Environments": [[1, "environments"]], "Flappy Bird": [[1, "flappy-bird"]], "Action Space:": [[1, "action-space"]], "Reward:": [[1, "reward"]], "Observation:": [[1, "observation"]], "Debug:": [[1, "debug"]], "Example:": [[1, "example"]], "Welcome to RLib\u2019s documentation!": [[2, "welcome-to-rlib-s-documentation"]], "Contents:": [[2, null]], "Indices and tables": [[2, "indices-and-tables"]], "Learning": [[3, "learning"]], "Algorithms Available": [[3, "algorithms-available"]], "Usage": [[3, "usage"]], "BaseAlgorithm": [[3, "basealgorithm"]], "EvolutionStrategy": [[3, "evolutionstrategy"]], "QLearning": [[3, "qlearning"]], "Deep Q-Learning": [[3, "deep-q-learning"]], "Deep Deterministic Policy Gradient": [[3, "deep-deterministic-policy-gradient"]]}, "indexentries": {"mlp (class in agents.mlp)": [[0, "agents.mlp.MLP"]], "qtable (class in agents.q_table)": [[0, "agents.q_table.QTable"]], "__init__() (agents.mlp.mlp method)": [[0, "agents.mlp.MLP.__init__"]], "__init__() (agents.q_table.qtable method)": [[0, "agents.q_table.QTable.__init__"]], "discretize() (agents.q_table.qtable method)": [[0, "agents.q_table.QTable.discretize"]], "get_action() (agents.q_table.qtable method)": [[0, "agents.q_table.QTable.get_action"]], "get_agent() (in module rlib.agents)": [[0, "rlib.agents.get_agent"]], "sample() (agents.q_table.qtable method)": [[0, "agents.q_table.QTable.sample"]], "update() (agents.q_table.qtable method)": [[0, "agents.q_table.QTable.update"]], "flappybirdenv (class in envs.flappy_bird_gymnasium.flappy_bird_env)": [[1, "envs.flappy_bird_gymnasium.flappy_bird_env.FlappyBirdEnv"]], "basealgorithm (class in learning.base_algorithm)": [[3, "learning.base_algorithm.BaseAlgorithm"]], "ddpg (class in learning.ddpg)": [[3, "learning.ddpg.DDPG"]], "deepqlearning (class in learning.deep_q_learning)": [[3, "learning.deep_q_learning.DeepQLearning"]], "evolutionstrategy (class in learning.evolution_strategy)": [[3, "learning.evolution_strategy.EvolutionStrategy"]], "qlearning (class in learning.q_learning)": [[3, "learning.q_learning.QLearning"]], "__init__() (learning.base_algorithm.basealgorithm method)": [[3, "learning.base_algorithm.BaseAlgorithm.__init__"]], "__init__() (learning.ddpg.ddpg method)": [[3, "learning.ddpg.DDPG.__init__"]], "__init__() (learning.deep_q_learning.deepqlearning method)": [[3, "learning.deep_q_learning.DeepQLearning.__init__"]], "__init__() (learning.evolution_strategy.evolutionstrategy method)": [[3, "learning.evolution_strategy.EvolutionStrategy.__init__"]], "__init__() (learning.q_learning.qlearning method)": [[3, "learning.q_learning.QLearning.__init__"]], "_get_random_parameters() (learning.evolution_strategy.evolutionstrategy method)": [[3, "learning.evolution_strategy.EvolutionStrategy._get_random_parameters"]], "_get_test_parameters() (learning.evolution_strategy.evolutionstrategy method)": [[3, "learning.evolution_strategy.EvolutionStrategy._get_test_parameters"]], "_parameters_update() (learning.evolution_strategy.evolutionstrategy method)": [[3, "learning.evolution_strategy.EvolutionStrategy._parameters_update"]], "_populate_replay_buffer() (learning.ddpg.ddpg method)": [[3, "learning.ddpg.DDPG._populate_replay_buffer"]], "_populate_replay_buffer() (learning.deep_q_learning.deepqlearning method)": [[3, "learning.deep_q_learning.DeepQLearning._populate_replay_buffer"]], "_update_target_weights() (learning.ddpg.ddpg method)": [[3, "learning.ddpg.DDPG._update_target_weights"]], "load() (learning.base_algorithm.basealgorithm class method)": [[3, "learning.base_algorithm.BaseAlgorithm.load"]], "save() (learning.base_algorithm.basealgorithm class method)": [[3, "learning.base_algorithm.BaseAlgorithm.save"]], "save_plots() (learning.base_algorithm.basealgorithm method)": [[3, "learning.base_algorithm.BaseAlgorithm.save_plots"]], "save_videos() (learning.base_algorithm.basealgorithm method)": [[3, "learning.base_algorithm.BaseAlgorithm.save_videos"]], "test() (learning.base_algorithm.basealgorithm method)": [[3, "learning.base_algorithm.BaseAlgorithm.test"]], "train() (learning.base_algorithm.basealgorithm method)": [[3, "learning.base_algorithm.BaseAlgorithm.train"]], "train_() (learning.base_algorithm.basealgorithm class method)": [[3, "learning.base_algorithm.BaseAlgorithm.train_"]], "update_weights() (learning.ddpg.ddpg method)": [[3, "learning.ddpg.DDPG.update_weights"]], "update_weights() (learning.deep_q_learning.deepqlearning method)": [[3, "learning.deep_q_learning.DeepQLearning.update_weights"]]}})